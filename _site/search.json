[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Coding Fundamentals for Humanists",
    "section": "",
    "text": "This workshop is part of the Digital Humanities Summer Institute (DHSI) 2024 event."
  },
  {
    "objectID": "about.html#instructors",
    "href": "about.html#instructors",
    "title": "About Coding Fundamentals for Humanists",
    "section": "Instructors",
    "text": "Instructors\n\n\n\n\n\n\n\n\n\nEvolutionary and behavioural ecologist by training, Software/Data Carpentry instructor, and open source advocate, Marie-Hélène Burle develops and delivers training for researchers on high-performance computing tools (R, Python, Julia, Git, Bash scripting, machine learning, parallel scientific programming, and HPC) for Simon Fraser University and the Digital Research Alliance of Canada.\n\n\n\n\n\n\n\n\n\n\n\n\nMeghan Landry recently joined ACENET as a Humanities & Social Sciences Research Specialist from St. Francis Xavier University, where she was a Scholarly Communications Librarian. In that role, she was very involved with the university’s strategic efforts in research data management and open access. She was responsible for implementing St. FX’s first institutional repository, StFX Scholar. Previously, she worked as a Digital Initiatives Librarian and managed the University of Prince Edward Island Library’s Virtual Research Environments (VREs) and Islandora repositories, which showcase digital collections and research related to PEI history. Meghan possesses an MLIS from McGill University and a BA in English Literature from UPEI. She is working towards a Technical Writing certification. Meghan is based at St. FX University but serves all of Atlantic Canada and is active in both national and regional humanities and social sciences initiatives."
  },
  {
    "objectID": "friday/index.html",
    "href": "friday/index.html",
    "title": "Project presentations",
    "section": "",
    "text": "Date:\nFriday June 14, 2024\nTime:\n9am–10:15am\n\n\n\nDon’t hesitate to use the etherpad for live comments."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Coding Fundamentals for Humanists",
    "section": "",
    "text": "MondayJune 10, 2024\n\n\n\n\nUsing the command line\nA brief introduction to Bash and the Unix shell\n\n\n\n\n\n\nTuesdayJune 11, 2024\n\n\n\n\nPython course\nBasics of the language\n\n\n\n\n\n\nWednesdayJune 12, 2024\n\n\n\n\nWeb scraping\nHow to automate data extraction from the web with Python\n\n\n\n\nAPI querying\nGathering internet data through APIs\n\n\n\n\n\n\nThursdayJune 13, 2024\n\n\n\n\nOn your own!\nA day to work on your projects (of course, we are here to help)\n\n\n\n\n\n\nFridayJune 14, 2024\n\n\n\n\nProject presentations\nTotally stress-free and casual presentations of your projects and what you learnt"
  },
  {
    "objectID": "monday/index.html",
    "href": "monday/index.html",
    "title": "Using the command line",
    "section": "",
    "text": "Date:\nMonday June 10, 2024\nTime:\n10:30am–noon, 1pm–2:30pm\nInstructor:\nMeghan Landry (ACENET)\n\n\n\nMaterial:\n\nEtherpad"
  },
  {
    "objectID": "thursday/index.html",
    "href": "thursday/index.html",
    "title": "On your own!",
    "section": "",
    "text": "Date:\nThursday June 13, 2024\nTime:\n9am–noon, 1pm–4pm\n\n\n\nDon’t hesitate to use the etherpad for live comments."
  },
  {
    "objectID": "tuesday/collections.html",
    "href": "tuesday/collections.html",
    "title": "Collections",
    "section": "",
    "text": "Values can be stored in collections. This section introduces tuples, dictionaries, sets, and arrays in Python."
  },
  {
    "objectID": "tuesday/collections.html#lists",
    "href": "tuesday/collections.html#lists",
    "title": "Collections",
    "section": "Lists",
    "text": "Lists\nLists are declared in square brackets:\n\nl = [2, 1, 3]\nl\n\n[2, 1, 3]\n\n\n\ntype(l)\n\nlist\n\n\nThey are mutable:\n\nl.append(0)\nl\n\n[2, 1, 3, 0]\n\n\nLists are ordered:\n\n['b', 'a'] == ['a', 'b']\n\nFalse\n\n\nThey can have repeat values:\n\n['a', 'a', 'a', 't']\n\n['a', 'a', 'a', 't']\n\n\nLists can be homogeneous:\n\n['b', 'a', 'x', 'e']\n\n['b', 'a', 'x', 'e']\n\n\n\ntype('b') == type('a') == type('x') == type('e')\n\nTrue\n\n\nor heterogeneous:\n\n[3, 'some string', 2.9, 'z']\n\n[3, 'some string', 2.9, 'z']\n\n\n\ntype(3) == type('some string') == type(2.9) == type('z')\n\nFalse\n\n\nThey can even be nested:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n\nThe length of a list is the number of items it contains and can be obtained with the function len:\n\nlen([3, ['b', 'e', 3.9, ['some string', 9.9]], 8])\n\n3\n\n\nTo extract an item from a list, you index it:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][0]\n\n3\n\n\n\nPython starts indexing at 0, so what we tend to think of as the “first” element of a list is for Python the “zeroth” element.\n\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][1]\n\n['b', 'e', 3.9, ['some string', 9.9]]\n\n\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][2]\n\n8\n\n\n\n# Of course you can't extract items that don't exist\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][3]\n\nIndexError: list index out of range\n\n\nYou can index from the end of the list with negative values (here you start at -1 for the last element):\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][-1]\n\n8\n\n\n\n\nYour turn:\n\nHow could you extract the string 'some string' from the list [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]?\n\nYou can also slice a list:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][0:1]\n\n[3]\n\n\n\nNotice how slicing returns a list.\nNotice also how the left index is included but the right index excluded.\n\nIf you omit the first index, the slice starts at the beginning of the list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][:6]\n\n[1, 2, 3, 4, 5, 6]\n\n\nIf you omit the second index, the slice goes to the end of the list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][6:]\n\n[7, 8, 9]\n\n\nWhen slicing, you can specify the stride:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][2:7:2]\n\n[3, 5, 7]\n\n\n\nThe default stride is 1:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][2:7] == [1, 2, 3, 4, 5, 6, 7, 8, 9][2:7:1]\n\nTrue\n\n\n\nYou can reverse the order of a list with a -1 stride applied on the whole list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][::-1]\n\n[9, 8, 7, 6, 5, 4, 3, 2, 1]\n\n\nYou can test whether an item is in a list:\n\n3 in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nTrue\n\n\n\n9 in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nFalse\n\n\nor not in a list:\n\n3 not in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nFalse\n\n\nYou can get the index (position) of an item inside a list:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8].index(3)\n\n0\n\n\n\nNote that this only returns the index of the first occurrence:\n\n[3, 3, ['b', 'e', 3.9, ['some string', 9.9]], 8].index(3)\n\n0\n\n\n\nLists are mutable (they can be modified). For instance, you can replace items in a list by other items:\n\nL = [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\nL\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n\n\nL[1] = 2\nL\n\n[3, 2, 8]\n\n\nYou can delete items from a list using their indices with list.pop:\n\nL.pop(2)\nL\n\n[3, 2]\n\n\n\nHere, because we are using list.pop, 2 represents the index (the 3rd item).\n\nor with del:\n\ndel L[0]\nL\n\n[2]\n\n\n\nNotice how a list can have a single item:\n\nlen(L)\n\n1\n\n\nIt is then called a “singleton list”.\n\nYou can also delete items from a list using their values with list.remove:\n\nL.remove(2)\nL\n\n[]\n\n\n\nHere, because we are using list.remove, 2 is the value 2.\n\n\nNotice how a list can even be empty:\n\nlen(L)\n\n0\n\n\nYou can actually initialise empty lists:\n\nM = []\ntype(M)\n\nlist\n\n\n\nYou can add items to a list. One at a time:\n\nL.append(7)\nL\n\n[7]\n\n\nAnd if you want to add multiple items at once?\n\n# This doesn't work...\nL.append(3, 6, 9)\n\nTypeError: list.append() takes exactly one argument (3 given)\n\n\n\n# This doesn't work either (that's not what we wanted)\nL.append([3, 6, 9])\nL\n\n[7, [3, 6, 9]]\n\n\n\n\nYour turn:\n\nFix this mistake we just made and remove the nested list [3, 6, 9].\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOne option is:\n\ndel L[1]\n\n\n\n\nTo add multiple values to a list (and not a nested list), you need to use list.extend:\n\nL.extend([3, 6, 9])\nL\n\n[7, 3, 6, 9]\n\n\nIf you don’t want to add an item at the end of a list, you can use list.insert(&lt;index&gt;, &lt;object&gt;):\n\nL.insert(3, 'test')\nL\n\n[7, 3, 6, 'test', 9]\n\n\n\n\nYour turn:\n\nLet’s have the following list:\n\nL = [7, [3, 6, 9], 3, 'test', 6, 9]\n\nInsert the string 'nested' in the zeroth position of the nested list [3, 6, 9] in L.\n\nYou can sort an homogeneous list:\n\nL = [3, 9, 10, 0]\nL.sort()\nL\n\n[0, 3, 9, 10]\n\n\n\nL = ['some string', 'b', 'a']\nL.sort()\nL\n\n['a', 'b', 'some string']\n\n\n\nHeterogeneous lists cannot be sorted:\n\nL = [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\nL.sort()\n\nTypeError: '&lt;' not supported between instances of 'list' and 'int'\n\n\n\nYou can also get the min and max value of homogeneous lists:\n\nmin([3, 9, 10, 0])\n\n0\n\n\n\nmax(['some string', 'b', 'a'])\n\n'some string'\n\n\n\nFor heterogeneous lists, this also doesn’t work:\n\nmin([3, ['b', 'e', 3.9, ['some string', 9.9]], 8])\n\nTypeError: '&lt;' not supported between instances of 'list' and 'int'\n\n\n\nLists can be concatenated with +:\n\nL + [3, 6, 9]\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8, 3, 6, 9]\n\n\nor repeated with *:\n\nL * 3\n\n[3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8,\n 3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8,\n 3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8]\n\n\n\nTo sum up, lists are declared in square brackets. They are mutable, ordered (thus indexable), and possibly heterogeneous collections of values."
  },
  {
    "objectID": "tuesday/collections.html#strings",
    "href": "tuesday/collections.html#strings",
    "title": "Collections",
    "section": "Strings",
    "text": "Strings\nStrings behave (a little) like lists of characters in that they have a length (the number of characters):\n\nS = 'This is a string.'\nlen(S)\n\n17\n\n\nThey have a min and a max:\n\nmin(S)\n\n' '\n\n\n\nmax(S)\n\n't'\n\n\nYou can index them:\n\nS[3]\n\n's'\n\n\nSlice them:\n\nS[10:16]\n\n'string'\n\n\n\n\nYour turn:\n\nReverse the order of the string S.\n\nThey can also be concatenated with +:\n\nT = 'This is another string.'\nprint(S + ' ' + T)\n\nThis is a string. This is another string.\n\n\nor repeated with *:\n\nprint(S * 3)\n\nThis is a string.This is a string.This is a string.\n\n\n\nprint((S + ' ') * 3)\n\nThis is a string. This is a string. This is a string. \n\n\nThis is where the similarities stop however: methods such as list.sort, list.append, etc. will not work on strings."
  },
  {
    "objectID": "tuesday/collections.html#arrays",
    "href": "tuesday/collections.html#arrays",
    "title": "Collections",
    "section": "Arrays",
    "text": "Arrays\nPython comes with a built-in array module. When you need arrays for storing and retrieving data, this module is perfectly suitable and extremely lightweight. This tutorial covers the syntax in detail.\nWhenever you plan on performing calculations on your data however (which is the vast majority of cases), you should instead use the NumPy package, covered in another section."
  },
  {
    "objectID": "tuesday/collections.html#tuples",
    "href": "tuesday/collections.html#tuples",
    "title": "Collections",
    "section": "Tuples",
    "text": "Tuples\nTuples are defined with parentheses:\n\nt = (3, 1, 4, 2)\nt\n\n(3, 1, 4, 2)\n\n\n\ntype(t)\n\ntuple\n\n\nTuples are ordered:\n\n(2, 3) == (3, 2)\n\nFalse\n\n\nThis means that they are indexable and sliceable:\n\n(2, 4, 6)[2]\n\n6\n\n\n\n(2, 4, 6)[::-1]\n\n(6, 4, 2)\n\n\nThey can be nested:\n\ntype((3, 1, (0, 2)))\n\ntuple\n\n\n\nlen((3, 1, (0, 2)))\n\n3\n\n\n\nmax((3, 1, 2))\n\n3\n\n\nThey can be heterogeneous:\n\ntype(('string', 2, True))\n\ntuple\n\n\nYou can create empty tuples:\n\ntype(())\n\ntuple\n\n\nYou can also create singleton tuples, but the syntax is a bit odd:\n\n# This is not a tuple...\ntype((1))\n\nint\n\n\n\n# This is the weird way to define a singleton tuple\ntype((1,))\n\ntuple\n\n\nHowever, the big difference with lists is that tuples are immutable:\n\nT = (2, 5)\nT[0] = 8\n\nTypeError: 'tuple' object does not support item assignment\n\n\nTuples are quite fascinating:\n\na, b = 1, 2\na, b\n\n(1, 2)\n\n\n\na, b = b, a\na, b\n\n(2, 1)\n\n\n\nTuples are declared in parentheses. They are immutable, ordered (thus indexable), and possibly heterogeneous collections of values."
  },
  {
    "objectID": "tuesday/collections.html#sets",
    "href": "tuesday/collections.html#sets",
    "title": "Collections",
    "section": "Sets",
    "text": "Sets\nSets are declared in curly braces:\n\ns = {3, 2, 5}\ns\n\n{2, 3, 5}\n\n\n\ntype(s)\n\nset\n\n\nSets are unordered:\n\n{2, 4, 1} == {4, 2, 1}\n\nTrue\n\n\nConsequently, it makes no sense to index a set.\nSets can be heterogeneous:\n\nS = {2, 'a', 'string'}\nisinstance(S, set)\n\nTrue\n\n\n\ntype(2) == type('a') == type('string')\n\nFalse\n\n\nThere are no duplicates in a set:\n\n{2, 2, 'a', 2, 'string', 'a'}\n\n{2, 'a', 'string'}\n\n\nYou can define an empty set, but only with the set function (because empty curly braces define a dictionary):\n\nt = set()\nt\n\nset()\n\n\n\nlen(t)\n\n0\n\n\n\ntype(t)\n\nset\n\n\nSince strings an iterables, you can use set to get a set of the unique characters:\n\nset('abba')\n\n{'a', 'b'}\n\n\n\n\nYour turn:\n\nHow could you create a set with the single element 'abba' in it?\n\n\nSets are declared in curly brackets. They are mutable, unordered (thus non indexable), possibly heterogeneous collections of unique values."
  },
  {
    "objectID": "tuesday/collections.html#dictionaries",
    "href": "tuesday/collections.html#dictionaries",
    "title": "Collections",
    "section": "Dictionaries",
    "text": "Dictionaries\nDictionaries are declared in curly braces. They associate values to keys:\n\nd = {'key1': 'value1', 'key2': 'value2'}\nd\n\n{'key1': 'value1', 'key2': 'value2'}\n\n\n\ntype(d)\n\ndict\n\n\nDictionaries are unordered:\n\n{'a': 1, 'b': 2} == {'b': 2, 'a': 1}\n\nTrue\n\n\nConsequently, the pairs themselves cannot be indexed. However, you can access values in a dictionary from their keys:\n\nD = {'c': 1, 'a': 3, 'b': 2}\nD['b']\n\n2\n\n\n\nD.get('b')\n\n2\n\n\n\nD.items()\n\ndict_items([('c', 1), ('a', 3), ('b', 2)])\n\n\n\nD.values()\n\ndict_values([1, 3, 2])\n\n\n\nD.keys()\n\ndict_keys(['c', 'a', 'b'])\n\n\nTo return a sorted list of keys:\n\nsorted(D)\n\n['a', 'b', 'c']\n\n\nYou can create empty dictionaries:\n\nE = {}\ntype(E)\n\ndict\n\n\nDictionaries are mutable, so you can add, remove, or replace items.\nLet’s add an item to our empty dictionary E:\n\nE['author'] = 'Proust'\nE\n\n{'author': 'Proust'}\n\n\nWe can add another one:\n\nE['title'] = 'In search of lost time'\nE\n\n{'author': 'Proust', 'title': 'In search of lost time'}\n\n\nWe can modify one:\n\nE['author'] = 'Marcel Proust'\nE\n\n{'author': 'Marcel Proust', 'title': 'In search of lost time'}\n\n\n\n\nYour turn:\n\nAdd a third item to E with the number of volumes.\n\nWe can also remove items:\n\nE.pop('author')\nE\n\n{'title': 'In search of lost time'}\n\n\nAnother method to remove items:\n\ndel E['title']\nE\n\n{}\n\n\n\nDictionaries are declared in curly braces. They are mutable and unordered collections of key/value pairs. They play the role of an associative array."
  },
  {
    "objectID": "tuesday/collections.html#conversion-between-collections",
    "href": "tuesday/collections.html#conversion-between-collections",
    "title": "Collections",
    "section": "Conversion between collections",
    "text": "Conversion between collections\nFrom tuple to list:\n\nlist((3, 8, 1))\n\n[3, 8, 1]\n\n\nFrom tuple to set:\n\nset((3, 2, 3, 3))\n\n{2, 3}\n\n\nFrom list to tuple:\n\ntuple([3, 1, 4])\n\n(3, 1, 4)\n\n\nFrom list to set:\n\nset(['a', 2, 4])\n\n{2, 4, 'a'}\n\n\nFrom set to tuple:\n\ntuple({2, 3})\n\n(2, 3)\n\n\nFrom set to list:\n\nlist({2, 3})\n\n[2, 3]"
  },
  {
    "objectID": "tuesday/collections.html#collections-module",
    "href": "tuesday/collections.html#collections-module",
    "title": "Collections",
    "section": "Collections module",
    "text": "Collections module\nPython has a built-in collections module providing the additional data structures: deque, defaultdict, namedtuple, OrderedDict, Counter, ChainMap, UserDict, UserList, and UserList."
  },
  {
    "objectID": "tuesday/control_flow.html",
    "href": "tuesday/control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Control flow statements alter the linear execution of code, allowing for one or another section of code to be executed, or for one section of code to be executed multiple times."
  },
  {
    "objectID": "tuesday/control_flow.html#conditionals",
    "href": "tuesday/control_flow.html#conditionals",
    "title": "Control flow",
    "section": "Conditionals",
    "text": "Conditionals\nConditionals dictate the flow of information based on predicates (statements that return True or False).\n\nExample predicates:\n\n4 &lt; 3\n2 == 4\n2 != 4\n2 in range(5)\n2 not in range(5)\n3 &lt;= 4 and 4 &gt; 5\n3 &lt;= 4 and 4 &gt; 5 and 3 != 2\n3 &lt;= 4 or 4 &gt; 5\n\nIf statements\nIn the simplest case, we have:\nif &lt;predicate&gt;:\n    &lt;some action&gt;\nThis translates to:\n\nIf &lt;predicate&gt; evaluates to True, the body of the if statement gets evaluated (&lt;some action&gt; is run),\nIf &lt;predicate&gt; evaluates to False, nothing happens.\n\n\nExamples:\n\n\nx = 3\nif x &gt;= 0:\n    print(x, 'is positive')\n\n3 is positive\n\n\n\nx = -3\nif x &gt;= 0:\n    print(x, 'is positive')\n\n\nNothing gets returned since the predicate returned False.\n\n\n\nIf else statements\nLet’s add an else statement so that our code also returns something when the predicate evaluates to False:\nif &lt;predicate&gt;:\n    &lt;some action&gt;\nelse:\n    &lt;some other action&gt;\n\nExample:\n\n\nx = -3\nif x &gt;= 0:\n    print(x, 'is positive')\nelse:\n    print(x, 'is negative')\n\n-3 is negative\n\n\n\n\nIf elif else\nWe can make this even more complex with:\nif &lt;predicate1&gt;:\n    &lt;some action&gt;\nelif &lt;predicate2&gt;:\n    &lt;some other action&gt;    \nelse:\n    &lt;yet some other action&gt;\n\nExample:\n\n\nx = -3\nif x &gt; 0:\n    print(x, 'is positive')\nelif x &lt; 0:\n    print(x, 'is negative')\nelse:\n    print(x, 'is zero')\n\n-3 is negative"
  },
  {
    "objectID": "tuesday/control_flow.html#loops",
    "href": "tuesday/control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\n\nFor loops\nFor loops run a set of instructions for each element of an iterable.\nAn iterable is any Python object cable of returning the items it contains one at a time.\n\nExamples of iterables:\n\nrange(5)\n'a string is an iterable'\n[2, 'word', 4.0]\nFor loops follow the syntax:\nfor &lt;iterable&gt;:\n    &lt;some action&gt;\n\nExample:\n\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\n\nYour turn:\n\nRemember that the indentation matters in Python.\nWhat do you think that this will print?\nfor i in range(5):\n    print(i)\nprint(i)\n\nStrings are iterables too, so this works:\n\nfor i in 'a string is an iterable':\n    print(i)\n\na\n \ns\nt\nr\ni\nn\ng\n \ni\ns\n \na\nn\n \ni\nt\ne\nr\na\nb\nl\ne\n\n\nTo iterate over multiple iterables at the same time, a convenient option is to use the function zip which creates an iterator of tuples:\n\nfor i, j in zip([1, 2, 3, 4], [3, 4, 5, 6]):\n    print(i + j)\n\n4\n6\n8\n10\n\n\n\n\nWhile loops\nWhile loops run as long as a predicate remains true. They follow the syntax:\nwhile &lt;predicate&gt;:\n    &lt;some action&gt;\n\nExample:\n\n\ni = 0\nwhile i &lt;= 10:\n    print(i)\n    i += 1\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10"
  },
  {
    "objectID": "tuesday/functions.html",
    "href": "tuesday/functions.html",
    "title": "Writing functions",
    "section": "",
    "text": "Python comes with a number of built-in functions. Packages can provide additional ones. In many cases however, you will want to create your own functions to perform exactly the computations that you need.\nIn this section, we will see how to define new functions."
  },
  {
    "objectID": "tuesday/functions.html#syntax",
    "href": "tuesday/functions.html#syntax",
    "title": "Writing functions",
    "section": "Syntax",
    "text": "Syntax\nThe function definition syntax follows:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    &lt;body&gt;\nOnce defined, new functions can be used as any other function.\nLet’s give this a try by creating some greeting functions."
  },
  {
    "objectID": "tuesday/functions.html#function-without-argument",
    "href": "tuesday/functions.html#function-without-argument",
    "title": "Writing functions",
    "section": "Function without argument",
    "text": "Function without argument\nLet’s start with the simple case in which our function does not accept any argument:\n\ndef hello():\n    print('Hello')\n\nThen we call it:\n\nhello()\n\nHello\n\n\nThis was great, but …\n\nhello('Marie')\n\nTypeError: hello() takes 0 positional arguments but 1 was given\n\n\n… it does not accept arguments."
  },
  {
    "objectID": "tuesday/functions.html#function-with-one-argument",
    "href": "tuesday/functions.html#function-with-one-argument",
    "title": "Writing functions",
    "section": "Function with one argument",
    "text": "Function with one argument\nLet’s step this up with a function which can accept an argument:\n\ndef greetings(name):\n    print('Hello ' + name)\n\nThis time, this works:\n\ngreetings('Marie')\n\nHello Marie\n\n\nHowever, this does not work anymore:\n\ngreetings()\n\nTypeError: greetings() missing 1 required positional argument: 'name'\n\n\n:("
  },
  {
    "objectID": "tuesday/functions.html#function-with-a-facultative-argument",
    "href": "tuesday/functions.html#function-with-a-facultative-argument",
    "title": "Writing functions",
    "section": "Function with a facultative argument",
    "text": "Function with a facultative argument\nLet’s make this even more fancy: a function with a facultative argument. That is, a function which accepts an argument, but also has a default value for when we do not provide any argument:\n\ndef howdy(name='you'):\n    print('Hello ' + name)\n\nWe can call it without argument (making use of the default value):\n\nhowdy()\n\nHello you\n\n\nAnd we can call it with an argument:\n\nhowdy('Marie')\n\nHello Marie\n\n\nThis was better, but …\n\nhowdy('Marie', 'Paul')\n\nTypeError: howdy() takes from 0 to 1 positional arguments but 2 were given\n\n\n… this does not work."
  },
  {
    "objectID": "tuesday/functions.html#function-with-two-arguments",
    "href": "tuesday/functions.html#function-with-two-arguments",
    "title": "Writing functions",
    "section": "Function with two arguments",
    "text": "Function with two arguments\nWe could create a function which takes two arguments:\n\ndef hey(name1, name2):\n    print('Hello ' + name1 + ', ' + name2)\n\nWhich solves our problem:\n\nhey('Marie', 'Paul')\n\nHello Marie, Paul\n\n\nBut it is terribly limiting:\n\n# This doesn't work\nhey()\n\nTypeError: hey() missing 2 required positional arguments: 'name1' and 'name2'\n\n\n\n# And neither does this\nhey('Marie')\n\nTypeError: hey() missing 1 required positional argument: 'name2'\n\n\n\n# Nor to mention this...\nhey('Marie', 'Paul', 'Alex')\n\nTypeError: hey() takes 2 positional arguments but 3 were given"
  },
  {
    "objectID": "tuesday/functions.html#function-with-any-number-of-arguments",
    "href": "tuesday/functions.html#function-with-any-number-of-arguments",
    "title": "Writing functions",
    "section": "Function with any number of arguments",
    "text": "Function with any number of arguments\nLet’s create a truly great function which handles all our cases:\n\ndef hi(name='you', *args):\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\nAnd let’s test it:\n\nhi()\nhi('Marie')\nhi('Marie', 'Paul')\nhi('Marie', 'Paul', 'Alex')\n\nHello you\nHello Marie\nHello Marie, Paul\nHello Marie, Paul, Alex\n\n\nEverything works!"
  },
  {
    "objectID": "tuesday/functions.html#documenting-functions",
    "href": "tuesday/functions.html#documenting-functions",
    "title": "Writing functions",
    "section": "Documenting functions",
    "text": "Documenting functions\nIt is a good habit to document what your functions do. As with comments, those “documentation strings” or “docstrings” will help future you or other users of your code.\nPEP 257—docstring conventions—suggests to use single-line docstrings surrounded by triple quotes.\nRemember the function definition syntax we saw at the start of this chapter? To be more exhaustive, we should have written it this way:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    \"\"\"&lt;docstrings&gt;\"\"\"\n    &lt;body&gt;\n\nExample:\n\n\ndef hi(name='you', *args):\n    \"\"\"Print a greeting\"\"\"\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\nPEP 8—the style guide for Python code—suggests a maximum of 72 characters per line for docstrings.\nIf your docstring is longer, you should create a multi-line one. In that case, PEP 257 suggests to have a summary line at the top (right after the opening set of triple quotes), then leave a blank line, then have your long docstrings (which can occupy multiple lines), and finally have the closing set of triple quotes on a line of its own:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    \"\"\"&lt;summary docstrings line&gt;\"\"\"\n\n    &lt;more detailed description&gt;\n    \"\"\"\n    &lt;body&gt;\n\nExample:\n\n\ndef hi(name='you', *args):\n    \"\"\"Print a greeting\n\n    Accepts any number of arguments\n    \"\"\"\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\n\nYou can (and should) document modules, classes, and methods in the same way.\n\nYou can now access the documentation of your function as you would any Python function:\n\nhelp(hi)\n\nHelp on function hi in module __main__:\n\nhi(name='you', *args)\n    Print a greeting\n    \n    Accepts any number of arguments\n\n\n\nOr:\n\nprint(hi.__doc__)\n\nPrint a greeting\n\n    Accepts any number of arguments"
  },
  {
    "objectID": "tuesday/index.html",
    "href": "tuesday/index.html",
    "title": "Python course",
    "section": "",
    "text": "Date:\nTuesday June 11, 2024\nTime:\n9am–noon, 1pm–4pm\nInstructors:\nMarie-Hélène Burle (Simon Fraser University) & Meghan Landry (ACENET)\n\n Start the afternoon session here ➤",
    "crumbs": [
      "Python course"
    ]
  },
  {
    "objectID": "tuesday/pandas.html",
    "href": "tuesday/pandas.html",
    "title": "DataFrames with Pandas",
    "section": "",
    "text": "pandas is a Python library built to manipulate data frames and time series.\nFor this section, we will use the Covid-19 data from the Johns Hopkins University CSSE repository.\nYou can visualize this data in a dashboard created by the Johns Hopkins University Center for Systems Science and Engineering."
  },
  {
    "objectID": "tuesday/pandas.html#setup",
    "href": "tuesday/pandas.html#setup",
    "title": "DataFrames with Pandas",
    "section": "Setup",
    "text": "Setup\nFirst, we need to load the pandas library and read in the data from the web:\n\n# Load the pandas library and create a shorter name for it\nimport pandas as pd\n\n# The global confirmed cases are available in CSV format at the url:\nurl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n\n# pandas allows to read in data from the web directly\ncases = pd.read_csv(url)"
  },
  {
    "objectID": "tuesday/pandas.html#first-look-at-the-data",
    "href": "tuesday/pandas.html#first-look-at-the-data",
    "title": "DataFrames with Pandas",
    "section": "First look at the data",
    "text": "First look at the data\nWhat does our data look like?\n\ncases\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\nLat\nLong\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n0\nNaN\nAfghanistan\n33.939110\n67.709953\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\n1\nNaN\nAlbania\n41.153300\n20.168300\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\n2\nNaN\nAlgeria\n28.033900\n1.659600\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\n3\nNaN\nAndorra\n42.506300\n1.521800\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\n4\nNaN\nAngola\n-11.202700\n17.873900\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284\nNaN\nWest Bank and Gaza\n31.952200\n35.233200\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\n285\nNaN\nWinter Olympics 2022\n39.904200\n116.407400\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\n286\nNaN\nYemen\n15.552727\n48.516388\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\n287\nNaN\nZambia\n-13.133897\n27.849332\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\n288\nNaN\nZimbabwe\n-19.015438\n29.154857\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n289 rows × 1147 columns\n\n\n\n\n# Quick summary of the data\ncases.describe()\n\n\n\n\n\n\n\n\nLat\nLong\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\ncount\n287.000000\n287.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n...\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n\n\nmean\n19.718719\n22.182084\n1.927336\n2.273356\n3.266436\n4.972318\n7.335640\n10.134948\n19.307958\n21.346021\n...\n2.336755e+06\n2.337519e+06\n2.338173e+06\n2.338805e+06\n2.338992e+06\n2.339187e+06\n2.339387e+06\n2.339839e+06\n2.340460e+06\n2.341073e+06\n\n\nstd\n25.956609\n77.870931\n26.173664\n26.270191\n32.707271\n45.523871\n63.623197\n85.724481\n210.329649\n211.628535\n...\n8.506608e+06\n8.511285e+06\n8.514488e+06\n8.518031e+06\n8.518408e+06\n8.518645e+06\n8.519346e+06\n8.521641e+06\n8.524968e+06\n8.527765e+06\n\n\nmin\n-71.949900\n-178.116500\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n25%\n4.072192\n-32.823050\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n\n\n50%\n21.512583\n20.939400\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n\n\n75%\n40.401784\n89.224350\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.051998e+06\n1.052122e+06\n1.052247e+06\n1.052382e+06\n1.052519e+06\n1.052664e+06\n1.052664e+06\n1.052926e+06\n1.053068e+06\n1.053213e+06\n\n\nmax\n71.706900\n178.065000\n444.000000\n444.000000\n549.000000\n761.000000\n1058.000000\n1423.000000\n3554.000000\n3554.000000\n...\n1.034435e+08\n1.035339e+08\n1.035898e+08\n1.036487e+08\n1.036508e+08\n1.036470e+08\n1.036555e+08\n1.036909e+08\n1.037558e+08\n1.038027e+08\n\n\n\n\n8 rows × 1145 columns\n\n\n\n\nOf course, this value is meaningless for Lat and Long!\n\n\n# Data types of the various columns\ncases.dtypes\n\nProvince/State     object\nCountry/Region     object\nLat               float64\nLong              float64\n1/22/20             int64\n                   ...   \n3/5/23              int64\n3/6/23              int64\n3/7/23              int64\n3/8/23              int64\n3/9/23              int64\nLength: 1147, dtype: object\n\n\n\ncases.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 289 entries, 0 to 288\nColumns: 1147 entries, Province/State to 3/9/23\ndtypes: float64(2), int64(1143), object(2)\nmemory usage: 2.5+ MB\n\n\n\ncases.shape\n\n(289, 1147)"
  },
  {
    "objectID": "tuesday/pandas.html#cases-per-country-by-date",
    "href": "tuesday/pandas.html#cases-per-country-by-date",
    "title": "DataFrames with Pandas",
    "section": "Cases per country by date",
    "text": "Cases per country by date\nThe dataset is a time series: this means that we have the cumulative numbers up to each date.\n\n# Let's get rid of the latitude and longitude to simplify our data\nsimple = cases.drop(columns=['Lat', 'Long'])\nsimple\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n0\nNaN\nAfghanistan\n0\n0\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\n1\nNaN\nAlbania\n0\n0\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\n2\nNaN\nAlgeria\n0\n0\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\n3\nNaN\nAndorra\n0\n0\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\n4\nNaN\nAngola\n0\n0\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284\nNaN\nWest Bank and Gaza\n0\n0\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\n285\nNaN\nWinter Olympics 2022\n0\n0\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\n286\nNaN\nYemen\n0\n0\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\n287\nNaN\nZambia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\n288\nNaN\nZimbabwe\n0\n0\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n289 rows × 1145 columns\n\n\n\n Some countries (e.g. Australia) are split between several provinces or states so we will have to add the values of all their provinces/states to get their totals.\nHere is how to make the sum for all Australian states:\nLet’s first select all the data for Australia: we want all the rows for which the Country/Region column is equal to Australia.\nFirst, we want to select the Country/Region column. There are several ways to index in pandas.\nWhen indexing columns, one can use square brackets directly after the DataFrame to index:\n\nsimple['Country/Region']\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\nHowever, it is more efficient to use the .loc or .iloc methods.\n\nUse .loc when using labels or booleans:\n\n\nsimple.loc[:, 'Country/Region']\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\n\nUse .iloc when using indices:\n\n\nsimple.iloc[:, 1]\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\n\nCountry/Region is the 2nd column, but indexing starts at 0 in Python.\n\nThen we need a conditional to filter the rows for which the value is equal to Australia:\n\nsimple.loc[:, 'Country/Region'] == 'Australia'\n\n0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n284    False\n285    False\n286    False\n287    False\n288    False\nName: Country/Region, Length: 289, dtype: bool\n\n\nFinally, we index, out of our entire data frame, the rows for which that condition returns True:\n\nsimple.loc[simple.loc[:, 'Country/Region'] == 'Australia']\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n9\nAustralian Capital Territory\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n232018\n232018\n232619\n232619\n232619\n232619\n232619\n232619\n232619\n232974\n\n\n10\nNew South Wales\nAustralia\n0\n0\n0\n0\n3\n4\n4\n4\n...\n3900969\n3900969\n3908129\n3908129\n3908129\n3908129\n3908129\n3908129\n3908129\n3915992\n\n\n11\nNorthern Territory\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n104931\n104931\n105021\n105021\n105021\n105021\n105021\n105021\n105021\n105111\n\n\n12\nQueensland\nAustralia\n0\n0\n0\n0\n0\n0\n0\n1\n...\n1796633\n1796633\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n\n\n13\nSouth Australia\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n880207\n880207\n881911\n881911\n881911\n881911\n881911\n881911\n881911\n883620\n\n\n14\nTasmania\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n286264\n286264\n286264\n286897\n286897\n286897\n286897\n286897\n286897\n287507\n\n\n15\nVictoria\nAustralia\n0\n0\n0\n0\n1\n1\n1\n1\n...\n2874262\n2874262\n2877260\n2877260\n2877260\n2877260\n2877260\n2877260\n2877260\n2880559\n\n\n16\nWestern Australia\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1291077\n1291077\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n\n\n\n\n8 rows × 1145 columns\n\n\n\n\nHere we use .loc to index based on a boolean array.\n\nWe can now make the sum for all of Australia for each day:\n\ntotal_australia = simple.loc[simple.loc[:, 'Country/Region'] == 'Australia'].sum(numeric_only=True)\ntotal_australia\n\n1/22/20           0\n1/23/20           0\n1/24/20           0\n1/25/20           0\n1/26/20           4\n             ...   \n3/5/23     11385534\n3/6/23     11385534\n3/7/23     11385534\n3/8/23     11385534\n3/9/23     11399460\nLength: 1143, dtype: int64\n\n\nWe can do this for all countries by grouping them:\n\ntotals = simple.groupby('Country/Region').sum(numeric_only=True)\ntotals\n\n\n\n\n\n\n\n\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n1/30/20\n1/31/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\nCountry/Region\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\nAlbania\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\nAlgeria\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\nAndorra\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\nAngola\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nWest Bank and Gaza\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\nWinter Olympics 2022\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\nYemen\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\nZambia\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\nZimbabwe\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n201 rows × 1143 columns\n\n\n\n Now, we can look at the totals for any date:\n\ntotals.loc[:, '6/12/21']\n\nCountry/Region\nAfghanistan              88740\nAlbania                 132449\nAlgeria                 133070\nAndorra                  13813\nAngola                   36600\n                         ...  \nWest Bank and Gaza      311018\nWinter Olympics 2022         0\nYemen                     6857\nZambia                  110332\nZimbabwe                 39852\nName: 6/12/21, Length: 201, dtype: int64\n\n\nTo make it easier to read, let’s order those numbers by decreasing order:\n\ntotals.loc[:, '6/12/21'].sort_values(ascending=False)\n\nCountry/Region\nUS                      33573694\nIndia                   29439989\nBrazil                  17385952\nFrance                   5799565\nTurkey                   5325435\n                          ...   \nPalau                          0\nWinter Olympics 2022           0\nKorea, North                   0\nSummer Olympics 2020           0\nTonga                          0\nName: 6/12/21, Length: 201, dtype: int64\n\n\nWe can also index the data for a particular country by indexing a row instead of a column:\n\ntotals.loc['Albania', :]\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     334427\n3/6/23     334427\n3/7/23     334427\n3/8/23     334443\n3/9/23     334457\nName: Albania, Length: 1143, dtype: int64\n\n\nWhen indexing rows, this syntax can be simplified to:\n\ntotals.loc['Albania']\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     334427\n3/6/23     334427\n3/7/23     334427\n3/8/23     334443\n3/9/23     334457\nName: Albania, Length: 1143, dtype: int64"
  },
  {
    "objectID": "tuesday/pandas.html#global-totals",
    "href": "tuesday/pandas.html#global-totals",
    "title": "DataFrames with Pandas",
    "section": "Global totals",
    "text": "Global totals\nNow, what if we want to have the world totals for each day? We calculate the columns totals (i.e. the sum across countries):\n\ntotals.sum()\n\n1/22/20          557\n1/23/20          657\n1/24/20          944\n1/25/20         1437\n1/26/20         2120\n             ...    \n3/5/23     676024901\n3/6/23     676082941\n3/7/23     676213378\n3/8/23     676392824\n3/9/23     676570149\nLength: 1143, dtype: int64\n\n\n\n\nYour turn:\n\nHow many confirmed cases were there in Venezuela by March 10, 2021?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, we need to select the data for Venezuela:\n\nvenez = totals.loc['Venezuela']\nvenez\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     552051\n3/6/23     552125\n3/7/23     552157\n3/8/23     552157\n3/9/23     552162\nName: Venezuela, Length: 1143, dtype: int64\n\n\nThen, we need to select for the proper date:\n\nanswer = venez.loc['3/10/21']\nanswer\n\n143321\n\n\nWe could have done it at once by indexing the row and column:\n\ntotals.loc['Venezuela', '3/10/21']\n\n143321"
  },
  {
    "objectID": "tuesday/pandas.html#pandas-documentation",
    "href": "tuesday/pandas.html#pandas-documentation",
    "title": "DataFrames with Pandas",
    "section": "Pandas documentation",
    "text": "Pandas documentation\n\nA user Guide to pandas\nFull documentation"
  },
  {
    "objectID": "wednesday_api/index.html",
    "href": "wednesday_api/index.html",
    "title": "API querying",
    "section": "",
    "text": "Date:\nWednesday June 12, 2024\nTime:\n1pm–4pm\nInstructor:\nMeghan Landry (ACENET)\n\n\n\nMaterial:\n\nEtherpad"
  },
  {
    "objectID": "wednesday_scraping/index.html",
    "href": "wednesday_scraping/index.html",
    "title": "Web scraping",
    "section": "",
    "text": "Date:\nWednesday June 12, 2024\nTime:\n9am–noon\nInstructor:\nMarie-Hélène Burle (Simon Fraser University)\nThe internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can however, be extremely tedious.\nWeb scraping tools allow to automate parts of that process and Python is a popular language for the task.\nIn this workshop, I will guide you through a simple example using the package Beautiful Soup."
  },
  {
    "objectID": "wednesday_scraping/index.html#html-and-css",
    "href": "wednesday_scraping/index.html#html-and-css",
    "title": "Web scraping",
    "section": "HTML and CSS",
    "text": "HTML and CSS\nHyperText Markup Language (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in Cascading Style Sheets (CSS) files.\nHTML uses tags of the form:\n&lt;some_tag&gt;Your content&lt;/some_tag&gt;\nSome tags have attributes:\n&lt;some_tag attribute_name=\"attribute value\"&gt;Your content&lt;/some_tag&gt;\n\nExamples:\n\nSite structure:\n\n&lt;h2&gt;This is a heading of level 2&lt;/h2&gt;\n&lt;p&gt;This is a paragraph&lt;/p&gt;\n\nFormatting:\n\n&lt;b&gt;This is bold&lt;/b&gt;\n&lt;a href=\"https://some.url\"&gt;This is the text for a link&lt;/a&gt;"
  },
  {
    "objectID": "wednesday_scraping/index.html#web-scrapping",
    "href": "wednesday_scraping/index.html#web-scrapping",
    "title": "Web scraping",
    "section": "Web scrapping",
    "text": "Web scrapping\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so remember to scrape responsibly."
  },
  {
    "objectID": "wednesday_scraping/index.html#goal-of-this-workshop",
    "href": "wednesday_scraping/index.html#goal-of-this-workshop",
    "title": "Web scraping",
    "section": "Goal of this workshop",
    "text": "Goal of this workshop\nWe will use a website from the University of Tennessee containing a database of PhD theses from that university.\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation.\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages."
  },
  {
    "objectID": "wednesday_scraping/index.html#lets-look-at-the-sites",
    "href": "wednesday_scraping/index.html#lets-look-at-the-sites",
    "title": "Web scraping",
    "section": "Let’s look at the sites",
    "text": "Let’s look at the sites\nFirst of all, let’s have a close look at the websites we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\nTo create a dataframe with the data for all the dissertations on that first page, we need to do two things:\n\nStep 1: from the dissertations database first page, we want to scrape the list of URLs for the dissertation pages.\nStep 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation."
  },
  {
    "objectID": "wednesday_scraping/index.html#load-packages",
    "href": "wednesday_scraping/index.html#load-packages",
    "title": "Web scraping",
    "section": "Load packages",
    "text": "Load packages\nLet’s load the packages that will make scraping websites with Python easier:\n\nimport requests                 # To download the html data from a site\nfrom bs4 import BeautifulSoup   # To parse the html data\nimport time                     # To add a delay between each requests\nimport pandas as pd             # To store our data in a DataFrame"
  },
  {
    "objectID": "wednesday_scraping/index.html#read-in-html-data-from-the-main-site",
    "href": "wednesday_scraping/index.html#read-in-html-data-from-the-main-site",
    "title": "Web scraping",
    "section": "Read in HTML data from the main site",
    "text": "Read in HTML data from the main site\nAs mentioned above, our site is the database of PhD dissertations from the University of Tennessee.\nLet’s create a string with the URL:\n\nurl = \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\nFirst, we send a request to that URL and save the response in a variable called r:\n\nr = requests.get(url)\n\nLet’s see what our response looks like:\n\nr\n\n&lt;Response [200]&gt;\n\n\nIf you look in the list of HTTP status codes, you can see that a response with a code of 200 means that the request was successful."
  },
  {
    "objectID": "wednesday_scraping/index.html#explore-the-raw-data",
    "href": "wednesday_scraping/index.html#explore-the-raw-data",
    "title": "Web scraping",
    "section": "Explore the raw data",
    "text": "Explore the raw data\nTo get the actual content of the response as unicode (text), we can use the text property of the response. This will give us the raw HTML markup from the webpage.\nLet’s print the first 200 characters:\n\nprint(r.text[:200])\n\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;&lt;!-- inj yui3-seed: --&gt;&lt;script type='text/javascript' src='//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js'&gt;&lt;/script&gt;&lt;script type='text/javascript' sr"
  },
  {
    "objectID": "wednesday_scraping/index.html#parse-the-data",
    "href": "wednesday_scraping/index.html#parse-the-data",
    "title": "Web scraping",
    "section": "Parse the data",
    "text": "Parse the data\nThe package Beautiful Soup transforms (parses) such HTML data into a parse tree, which will make extracting information easier.\nLet’s create an object called mainpage with the parse tree:\n\nmainpage = BeautifulSoup(r.text, \"html.parser\")\n\n\nhtml.parser is the name of the parser that we are using here. It is better to use a specific parser to get consistent results across environments.\n\nWe can print the beginning of the parsed result:\n\nprint(mainpage.prettify()[:200])\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n &lt;head&gt;\n  &lt;!-- inj yui3-seed: --&gt;\n  &lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js\" type=\"text/javascript\"&gt;\n  &lt;/script&gt;\n  &lt;script src=\"//ajax.g\n\n\n\nThe prettify method turns the BeautifulSoup object we created into a string (which is needed for slicing).\n\nIt doesn’t look any more clear to us, but it is now in a format the Beautiful Soup package can work with.\nFor instance, we can get the HTML segment containing the title with three methods:\n\nusing the title tag name:\n\n\nmainpage.title\n\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n\n\n\nusing find to look for HTML markers (tags, attributes, etc.):\n\n\nmainpage.find(\"title\")\n\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n\n\n\nusing select which accepts CSS selectors:\n\n\nmainpage.select(\"title\")\n\n[&lt;title&gt;\n Doctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n &lt;/title&gt;]\n\n\nfind will only return the first element. find_all will return all elements. select will also return all elements. Which one you chose depends on what you need to extract. There often several ways to get you there.\nHere are other examples of data extraction:\n\nmainpage.head\n\n&lt;head&gt;&lt;!-- inj yui3-seed: --&gt;&lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;!-- Adobe Analytics --&gt;&lt;script src=\"https://assets.adobedtm.com/4a848ae9611a/d0e96722185b/launch-d525bb0064d8.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;!-- Cookies --&gt;&lt;link href=\"//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;&lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"/assets/nr_browser_production.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;!-- def.1 --&gt;\n&lt;meta charset=\"utf-8\"/&gt;\n&lt;meta content=\"width=device-width\" name=\"viewport\"/&gt;\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n&lt;!-- FILE meta-tags.inc --&gt;&lt;!-- FILE: /srv/sequoia/main/data/assets/site/meta-tags.inc --&gt;\n&lt;!-- FILE: meta-tags.inc (cont) --&gt;\n&lt;!-- sh.1 --&gt;\n&lt;link href=\"/ir-style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-custom.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"ir-custom.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-local.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"ir-local.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-print.css\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/assets/floatbox/floatbox.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/recent.rss\" rel=\"alternate\" title=\"Site Feed\" type=\"application/rss+xml\"/&gt;\n&lt;link href=\"/favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\"/&gt;\n&lt;!--[if IE]&gt;\n&lt;link rel=\"stylesheet\" href=\"/ir-ie.css\" type=\"text/css\" media=\"screen\"&gt;\n&lt;![endif]--&gt;\n&lt;!-- JS --&gt;\n&lt;script src=\"/assets/jsUtilities.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/footnoteLinks.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/yui-init.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/bepress-init.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/JumpListYUI.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;!-- end sh.1 --&gt;\n&lt;script type=\"text/javascript\"&gt;var pageData = {\"page\":{\"environment\":\"prod\",\"productName\":\"bpdg\",\"language\":\"en\",\"name\":\"ir_etd\",\"businessUnit\":\"els:rp:st\"},\"visitor\":{}};&lt;/script&gt;\n&lt;/head&gt;\n\n\n\nmainpage.a\n\n&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;\n\n\n\nmainpage.find_all(\"a\")[:5]\n\n[&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/do/search/advanced/\" title=\"Search\"&gt;&lt;i class=\"icon-search\"&gt;&lt;/i&gt; Search&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/communities.html\" title=\"Browse\"&gt;Browse Collections&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"/cgi/myaccount.cgi?context=utk_graddiss\" title=\"My Account\"&gt;My Account&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/about.html\" title=\"About\"&gt;About&lt;/a&gt;]\n\n\n\nmainpage.select(\"a\")[:5]\n\n[&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/do/search/advanced/\" title=\"Search\"&gt;&lt;i class=\"icon-search\"&gt;&lt;/i&gt; Search&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/communities.html\" title=\"Browse\"&gt;Browse Collections&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"/cgi/myaccount.cgi?context=utk_graddiss\" title=\"My Account\"&gt;My Account&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/about.html\" title=\"About\"&gt;About&lt;/a&gt;]"
  },
  {
    "objectID": "wednesday_scraping/index.html#test-run",
    "href": "wednesday_scraping/index.html#test-run",
    "title": "Web scraping",
    "section": "Test run",
    "text": "Test run\n\nIdentify relevant markers\nThe html code for this webpage contains the data we are interested in, but it is mixed in with a lot of HTML formatting and data we don’t care about. We need to extract the data relevant to us and turn it into a workable format.\nThe first step is to find the HTML markers that contain our data. One option is to use a web inspector or—even easier—the SelectorGadget, a JavaScript bookmarklet built by Andrew Cantino.\nTo use this tool, go to the SelectorGadget website and drag the link of the bookmarklet to your bookmarks bar.\nNow, go to the dissertations database first page and click on the bookmarklet in your bookmarks bar. You will see a floating box at the bottom of your screen. As you move your mouse across the screen, an orange rectangle appears around each element over which you pass.\nClick on one of the dissertation links: now, there is an a appearing in the box at the bottom as well as the number of elements selected. The selected elements are highlighted in yellow. Those elements are links (in HTML, a tags define hyperlinks).\nAs you can see, all the links we want are selected. However, there are many other links we don’t want that are also highlighted. In fact, all links in the document are selected. We need to remove the categories of links that we don’t want. To do this, hover above any of the links we don’t want. You will see a red rectangle around it. Click on it: now all similar links are gone. You might have to do this a few times until only the relevant links (i.e. those that lead to the dissertation information pages) remain highlighted.\nAs there are 100 such links per page, the count of selected elements in the bottom floating box should be down to 100.\nIn the main section of the floating box, you can now see: .article-listing a. This means that the data we want are under the HTML elements .article-listing a (the class .article-listing and the tag a).\n\n\nExtract test URL\nIt is a good idea to test things out on a single element before doing a massive batch scraping of a site, so let’s test our method for the first dissertation.\nTo start, we need to extract the first URL. Here, we will use the CSS selectors (we can get there using find too). mainpage.select(\".article-listing a\") would give us all the results (100 links):\n\nlen(mainpage.select(\".article-listing a\"))\n\n100\n\n\nTo get the first one, we index it:\n\nmainpage.select(\".article-listing a\")[0]\n\n&lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8076\"&gt;Understanding host-microbe interactions in maize kernel and sweetpotato leaf metagenomic profiles.&lt;/a&gt;\n\n\nThe actual URL is contained in the href attribute. Attributes can be extracted with the get method:\n\nmainpage.select(\".article-listing a\")[0].get(\"href\")\n\n'https://trace.tennessee.edu/utk_graddiss/8076'\n\n\nWe now have our URL as a string. We can double-check that it is indeed a string:\n\ntype(mainpage.select(\".article-listing a\")[0].get(\"href\"))\n\nstr\n\n\nThis is exactly what we need to send a request to that site, so let’s create an object url_test with it:\n\nurl_test = mainpage.select(\".article-listing a\")[0].get(\"href\")\n\nWe have our first thesis URL:\n\nprint(url_test)\n\nhttps://trace.tennessee.edu/utk_graddiss/8076\n\n\n\n\nSend request to test URL\nNow that we have the URL for the first dissertation information page, we want to extract the date, major, and advisor for that dissertation.\nThe first thing to do—as we did earlier with the database site—is to send a request to that page. Let’s assign it to a new object that we will call r_test:\n\nr_test = requests.get(url_test)\n\nThen we can parse it with Beautiful Soup (as we did before). Let’s create a dissertpage_test object:\n\ndissertpage_test = BeautifulSoup(r_test.content, \"html.parser\")\n\n\n\nGet data for test URL\nIt is time to extract the publication date, major, and advisor for our test URL.\nLet’s start with the date. Thanks to the SelectorGadget, following the method we saw earlier, we can see that we now need elements marked by #publication_date p.\nWe can use select as we did earlier:\n\ndissertpage_test.select(\"#publication_date p\")\n\n[&lt;p&gt;5-2023&lt;/p&gt;]\n\n\nNotice the square brackets around our result: this is import. It shows us that we have a ResultSet (a list of results specific to Beautiful Soup). This is because select returns all the results. Here, we have a single result, but the format is still list-like. Before we can go further, we need to index the value out of it:\n\ndissertpage_test.select(\"#publication_date p\")[0]\n\n&lt;p&gt;5-2023&lt;/p&gt;\n\n\nWe can now get the text out of this paragraph with the text attribute:\n\ndissertpage_test.select(\"#publication_date p\")[0].text\n\n'5-2023'\n\n\nWe could save it in a variable date_test:\n\ndate_test = dissertpage_test.select(\"#publication_date p\")[0].text\n\n\n\nYour turn:\n\nGet the major and advisor for our test URL."
  },
  {
    "objectID": "wednesday_scraping/index.html#store-results-in-a-dataframe",
    "href": "wednesday_scraping/index.html#store-results-in-a-dataframe",
    "title": "Web scraping",
    "section": "Store results in a DataFrame",
    "text": "Store results in a DataFrame\nA DataFrame would be a lot more convenient than a list to hold our results.\nFirst, we create a list with the column names for our future DataFrame:\n\ncols = [\"Date\", \"Major\", \"PI\"]\n\nThen we create our DataFrame:\n\ndf = pd.DataFrame(ls, columns=cols)\n\n\ndf\n\n\n\n\n\n\n\n\nDate\nMajor\nPI\n\n\n\n\n0\n5-2023\nChemistry\nCraig E. Barnes\n\n\n1\n12-2022\nChemistry\nDr. Ampofo K. Darko\n\n\n2\n12-2022\nIndustrial Engineering\nJames Ostrowski\n\n\n3\n5-2022\nEntomology, Plant Pathology and Nematology\nHeather Kelly\n\n\n4\n5-2022\nMechanical Engineering\nCaleb D. Rucker\n\n\n...\n...\n...\n...\n\n\n95\n8-2022\nPsychology\nJacob L. Levy\n\n\n96\n8-2022\nMaterials Science and Engineering\nAlexei P. Sokolov\n\n\n97\n12-2022\nPhysics\nKate Jones\n\n\n98\n12-2022\nCivil Engineering\nJohn Z. Ma\n\n\n99\n5-2022\nEnergy Science and Engineering\nTomonori Saito\n\n\n\n\n100 rows × 3 columns"
  },
  {
    "objectID": "wednesday_scraping/index.html#functions-recap",
    "href": "wednesday_scraping/index.html#functions-recap",
    "title": "Web scraping",
    "section": "Functions recap",
    "text": "Functions recap"
  },
  {
    "objectID": "wednesday_scraping/index.html#full-run",
    "href": "wednesday_scraping/index.html#full-run",
    "title": "Web scraping",
    "section": "Full run",
    "text": "Full run\nOnce everything is working for a test site, we can do some bulk scraping.\n\nExtract all URLs\nWe already know how to get the 100 dissertations links from the main page: mainpage.select(\".article-listing a\"). Let’s assign it to a variable:\n\ndissertlinks = mainpage.select(\".article-listing a\")\n\nThis ResultSet is an iterable, meaning that it can be used in a loop.\nLet’s write a loop to extract all the URLs from this ResultSet of links:\n\nurls = []  # We create an empty list before filling it during the loop\n\nfor link in dissertlinks:\n    urls.append(link.get(\"href\"))\n\nLet’s see our first 5 URLs:\n\nurls[:5]\n\n['https://trace.tennessee.edu/utk_graddiss/8076',\n 'https://trace.tennessee.edu/utk_graddiss/9158',\n 'https://trace.tennessee.edu/utk_graddiss/8080',\n 'https://trace.tennessee.edu/utk_graddiss/8086',\n 'https://trace.tennessee.edu/utk_graddiss/8078']\n\n\n\n\nExtract data from each page\nFor each element of urls (i.e. for each dissertation URL), we can now get our information.\n\nls = []                                            # Create an empty list\n\nfor url in urls:                                   # For each element of our list of sites\n    r = requests.get(url)                                     # Send a request to the site\n    dissertpage = BeautifulSoup(r.text, \"html.parser\")        # Parse the result\n    date = dissertpage.select(\"#publication_date p\")[0].text  # Get the date\n    major = dissertpage.select(\"#department p\")[0].text       # Get the major\n    advisor = dissertpage.select(\"#advisor1 p\")[0].text            # Get the advisor\n    ls.append((date, major, advisor))                              # Store the results in the list\n    time.sleep(0.1)                                           # Add a delay at each iteration\n\n\nSome sites will block requests if they are too frequent. Adding a little delay between requests is often a good idea."
  },
  {
    "objectID": "wednesday_scraping/index.html#send-request-to-the-main-site",
    "href": "wednesday_scraping/index.html#send-request-to-the-main-site",
    "title": "Web scraping",
    "section": "Send request to the main site",
    "text": "Send request to the main site\nAs mentioned above, our site is the database of PhD dissertations from the University of Tennessee.\nLet’s create a string with the URL:\n\nurl = \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\nFirst, we send a request to that URL and save the response in a variable called r:\n\nr = requests.get(url)\n\nLet’s see what our response looks like:\n\nr\n\n&lt;Response [200]&gt;\n\n\nIf you look in the list of HTTP status codes, you can see that a response with a code of 200 means that the request was successful."
  },
  {
    "objectID": "wednesday_scraping/index.html#store-results-in-dataframe",
    "href": "wednesday_scraping/index.html#store-results-in-dataframe",
    "title": "Web scraping",
    "section": "Store results in DataFrame",
    "text": "Store results in DataFrame\nA DataFrame would be a lot more convenient than a list to hold our results.\nFirst, we create a list with the column names for our future DataFrame:\n\ncols = [\"Date\", \"Major\", \"Advisor\"]\n\nThen we create our DataFrame:\n\ndf = pd.DataFrame(ls, columns=cols)\n\n\ndf\n\n\n\n\n\n\n\n\n\nDate\nMajor\nAdvisor\n\n\n\n\n0\n5-2023\nLife Sciences\nBode A. Olukolu\n\n\n1\n12-2023\nIndustrial Engineering\nHugh Medal\n\n\n2\n5-2023\nNuclear Engineering\nErik Lukosi\n\n\n3\n5-2023\nEnergy Science and Engineering\nKyle R. Gluesenkamp\n\n\n4\n5-2023\nEnglish\nMargaret Lazarus Dean\n\n\n...\n...\n...\n...\n\n\n95\n8-2023\nEducational Psychology and Research\nQi Sun\n\n\n96\n12-2023\nNuclear Engineering\nLawrence H. Heilbronn\n\n\n97\n5-2023\nGeology\nBradley Thomson\n\n\n98\n5-2023\nNatural Resources\nSharon R. Jean-Philippe\n\n\n99\n12-2023\nPsychology\nGreg Stuart\n\n\n\n\n100 rows × 3 columns"
  },
  {
    "objectID": "wednesday_scraping/index.html#save-the-results-to-file",
    "href": "wednesday_scraping/index.html#save-the-results-to-file",
    "title": "Web scraping",
    "section": "Save the results to file",
    "text": "Save the results to file\nAs a final step, we will save our data to a CSV file:\n\ndf.to_csv('dissertations_data.csv', index=False)\n\n\nThe default index=True writes the row numbers. We are not writing these indices in our file by changing the value of this argument to False.\n\nIf you are using a Jupyter notebook or the IPython shell, you can type !ls to see that the file is there and !cat dissertations_data.csv to print its content.\n\n! is a magic command that allows to run Unix shell commands in a notebook or IPython shell."
  },
  {
    "objectID": "wednesday_scraping/index.html#example-for-this-workshop",
    "href": "wednesday_scraping/index.html#example-for-this-workshop",
    "title": "Web scraping",
    "section": "Example for this workshop",
    "text": "Example for this workshop\nWe will use a website from the University of Tennessee containing a database of PhD theses from that university.\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation.\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages."
  },
  {
    "objectID": "wednesday_scraping/index.html#save-results-to-file",
    "href": "wednesday_scraping/index.html#save-results-to-file",
    "title": "Web scraping",
    "section": "Save results to file",
    "text": "Save results to file\nAs a final step, we will save our data to a CSV file:\n\ndf.to_csv('dissertations_data.csv', index=False)\n\n\nThe default index=True writes the row numbers. We are not writing these indices in our file by changing the value of this argument to False.\n\nIf you are using a Jupyter notebook or the IPython shell, you can type !ls to see that the file is there and !cat dissertations_data.csv to print its content.\n\n! is a magic command that allows to run Unix shell commands in a notebook or IPython shell."
  }
]